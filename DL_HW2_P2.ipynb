{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws3FhzHt5hqg"
      },
      "source": [
        "# **Deep Learning Course**\n",
        "\n",
        "## **Loss Functions and Multilayer Perceptrons (MLP)**\n",
        "\n",
        "---\n",
        "\n",
        "### **Student Information:**\n",
        "\n",
        "- **Name:** *Pantea Amoie*\n",
        "- **Student Number:** *400101656*\n",
        "\n",
        "---\n",
        "\n",
        "### **Assignment Overview**\n",
        "\n",
        "In this notebook, we will explore various loss functions used in neural networks, with a specific focus on their role in training **Multilayer Perceptrons (MLPs)**. By the end of this notebook, you will have a deeper understanding of:\n",
        "- Types of loss functions\n",
        "- How loss functions affect the training process\n",
        "- The relationship between loss functions and model optimization in MLPs\n",
        "\n",
        "---\n",
        "\n",
        "### **Table of Contents**\n",
        "\n",
        "1. Introduction to Loss Functions\n",
        "2. Types of Loss Functions\n",
        "3. Multilayer Perceptrons (MLP)\n",
        "4. Implementing Loss Functions in MLP\n",
        "5. Conclusion\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9tEhU855hqh"
      },
      "source": [
        "# 1.Introduction to Loss Functions\n",
        "\n",
        "In deep learning, **loss functions** play a crucial role in training models by quantifying the difference between the predicted outputs and the actual targets. Selecting the appropriate loss function is essential for the success of your model. In this assay, we will explore various loss functions available in PyTorch, understand their theoretical backgrounds, and provide you with a scaffolded class to experiment with these loss functions.\n",
        "\n",
        "Before begining, let's train a simle MLP model using the **L1Loss** function. We'll return to this model later to experiment with different loss functions. We'll start by importing the necessary libraries and defining the model architecture.\n",
        "\n",
        "First things first, let's talk about **L1Loss**.\n",
        "\n",
        "### 1. L1Loss (`torch.nn.L1Loss`)\n",
        "- **Description:** Also known as Mean Absolute Error (MAE), L1Loss computes the average absolute difference between the predicted values and the target values.\n",
        "- **Use Case:** Suitable for regression tasks where robustness to outliers is desired.\n",
        "\n",
        "Here is the mathematical formulation of L1Loss:\n",
        "\\begin{equation}\n",
        "\\text{L1Loss} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{pred}_i} - y_{\\text{true}_i}|\n",
        "\\end{equation}\n",
        "\n",
        "Let's implement a simple MLP model using the L1Loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex0NKIXh5hqi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Don't be courious about Adam, it's just a fancy name for a fancy optimization algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_TaWd6H5hqi"
      },
      "source": [
        "Here, we'll define a class called `SimpleMLP` that inherits from `nn.Module`. This class can have multiple layers, and we'll use the `nn.Sequential` module to define the layers of the model. The model will have the following architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps40M3bH5hqi"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers=1, last_layer_activation_fn=nn.ReLU):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Define the layers of the MLP\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        if last_layer_activation_fn is not None:\n",
        "            layers.append(last_layer_activation_fn())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass of the MLP\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cJV--sX5hqi"
      },
      "source": [
        "Now, let's define a class called `SimpleMLP_Loss` that has the following architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSqAq9gB5hqi"
      },
      "outputs": [],
      "source": [
        "class SimpleMLPTrainer:\n",
        "    def __init__(self, model, criterion, optimizer):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def train(self, train_loader, num_epochs):\n",
        "        # Implement the training loop\n",
        "        # Note: You should also print the training loss at each epoch, use tqdm for progress bar\n",
        "        # Note: You should return the training loss at each epoch\n",
        "\n",
        "        self.model.train()\n",
        "        losses = []\n",
        "        for epoch in range(num_epochs):\n",
        "            current_loss = 0\n",
        "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "                inputs, targets = batch\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                current_loss += loss.item()\n",
        "\n",
        "            avg_loss = current_loss / len(train_loader)\n",
        "            losses.append(avg_loss)\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "    def evaluate(self, val_loader):\n",
        "        # Implement the evaluation loop\n",
        "        # Note: You should return the validation loss and accuracy\n",
        "\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                outputs = self.model(X_batch)\n",
        "                loss = self.criterion(outputs, y_batch)\n",
        "                total_loss += loss.item()\n",
        "                # For the third part of the question(when the number of output neurons is 2)\n",
        "                if outputs.shape[1] == 2:\n",
        "                    predictions = torch.argmax(outputs, dim=1)\n",
        "                    y_batch = torch.argmax(y_batch, dim=1) if y_batch.dim() > 1 else y_batch\n",
        "                else:\n",
        "                    predictions = (outputs >= 0.5).float().squeeze()\n",
        "                    y_batch = y_batch.squeeze()\n",
        "                # predictions = (outputs >= 0.5).float()\n",
        "                correct += (predictions == y_batch).sum().item()\n",
        "                total += y_batch.size(0)\n",
        "\n",
        "        val_loss = total_loss / len(val_loader)\n",
        "        accuracy = correct / total\n",
        "        return val_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK598_Vh5hqi"
      },
      "source": [
        "Next, lets test our model using the L1Loss function. You'll use <span style=\"color:red\">*Titanic Dataset*</span> to train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcv72PS15hqj"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "train_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "data = pd.read_csv(train_url)\n",
        "\n",
        "# Preprocessing (simple example)\n",
        "data = data[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']].dropna()\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Convert the data to PyTorch tensors and create a DataLoader\n",
        "X = data[['Pclass', 'Sex', 'Age', 'Fare']].values\n",
        "y = data['Survived'].values\n",
        "\n",
        "# We can scale the data\n",
        "# Without scaling we got an accuracy of around 62%\n",
        "# But after scaling, we observe around 15% increase!\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Define the model, criterion, and optimizer\n",
        "input_dim = X.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = 1\n",
        "num_hidden_layers = 2\n",
        "\n",
        "\n",
        "model = SimpleMLP(input_dim, hidden_dim, output_dim, num_hidden_layers, last_layer_activation_fn=nn.Sigmoid)\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXl3VKMr5hqj"
      },
      "source": [
        "<div style=\"text-align: center;\"> <span style=\"color:red; font-size: 26px; font-weight: bold;\">Let's train!</span> </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as2EUDQD5hqj",
        "outputId": "efda58a7-6e46-4204-a8c6-2a17c2f25eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 18/18 [00:00<00:00, 201.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.4700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 18/18 [00:00<00:00, 136.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: 0.3922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 18/18 [00:00<00:00, 239.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: 0.3055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 18/18 [00:00<00:00, 315.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: 0.2471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 18/18 [00:00<00:00, 86.31it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 0.2234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 18/18 [00:00<00:00, 213.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: 0.2127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 18/18 [00:00<00:00, 301.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: 0.2076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 18/18 [00:00<00:00, 290.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: 0.2008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 18/18 [00:00<00:00, 190.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: 0.1970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 18/18 [00:00<00:00, 218.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: 0.1943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 18/18 [00:00<00:00, 139.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: 0.1904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 18/18 [00:00<00:00, 116.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: 0.1874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 18/18 [00:00<00:00, 161.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: 0.1849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 18/18 [00:00<00:00, 299.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: 0.1853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 18/18 [00:00<00:00, 280.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: 0.1829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 18/18 [00:00<00:00, 121.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: 0.1824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 18/18 [00:00<00:00, 180.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: 0.1801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 18/18 [00:00<00:00, 341.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: 0.1813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 18/18 [00:00<00:00, 160.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: 0.1787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 18/18 [00:00<00:00, 93.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: 0.1783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.47002209226290387,\n",
              " 0.3921545810169644,\n",
              " 0.30547556032737094,\n",
              " 0.24706178489658567,\n",
              " 0.22343006605903307,\n",
              " 0.21271600988176134,\n",
              " 0.2076160336534182,\n",
              " 0.20078275062971646,\n",
              " 0.1970085096028116,\n",
              " 0.1942973170015547,\n",
              " 0.1904323668115669,\n",
              " 0.18742423670159447,\n",
              " 0.18488306303819022,\n",
              " 0.18528479689525235,\n",
              " 0.1829002220183611,\n",
              " 0.1823544195956654,\n",
              " 0.18006748002436426,\n",
              " 0.1813151886065801,\n",
              " 0.17866701053248513,\n",
              " 0.17833352709809938]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from torch.nn import L1Loss\n",
        "\n",
        "# Train the model\n",
        "criterion = L1Loss()\n",
        "trainer = SimpleMLPTrainer(model, criterion, optimizer)\n",
        "trainer.train(train_loader, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "val_loss, accuracy = trainer.evaluate(val_loader)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVnnokIRtfW6",
        "outputId": "7668e9fb-51d5-47ea-cdb1-b4cf76854246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2474, Accuracy: 0.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z36J4RGU5hqj"
      },
      "source": [
        "---\n",
        "# 2. Types of Loss Functions\n",
        "\n",
        "PyTorch offers a variety of built-in loss functions tailored for different types of problems, such as regression, classification, and more. Below, we discuss several commonly used loss functions, their theoretical foundations, and typical use cases.\n",
        "\n",
        "### 2. MSELoss (`torch.nn.MSELoss`)\n",
        "- **Description:** Mean Squared Error (MSE) calculates the average of the squares of the differences between predicted and target values.\n",
        "- **Use Case:** Commonly used in regression problems where larger errors are significantly penalized.\n",
        "\n",
        "Here is boring math stuff for MSE:\n",
        "\\begin{equation}\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}\n",
        "\\end{equation}\n",
        "\n",
        "<span style=\"color:red; font-size: 18px; font-weight: bold;\">Warning:</span> Don't forget to reinitialize the model before experimenting with different loss functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZAZ6XZu5hqj",
        "outputId": "29a2488e-4454-4bb2-b323-666fdabb7382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 18/18 [00:00<00:00, 533.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.2305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 18/18 [00:00<00:00, 472.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: 0.1890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 18/18 [00:00<00:00, 537.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: 0.1590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 18/18 [00:00<00:00, 516.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: 0.1460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 18/18 [00:00<00:00, 527.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 0.1417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 18/18 [00:00<00:00, 515.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: 0.1377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 18/18 [00:00<00:00, 477.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: 0.1362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 18/18 [00:00<00:00, 408.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: 0.1342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 18/18 [00:00<00:00, 247.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: 0.1326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 18/18 [00:00<00:00, 328.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: 0.1332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 18/18 [00:00<00:00, 313.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: 0.1322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 18/18 [00:00<00:00, 348.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: 0.1301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 18/18 [00:00<00:00, 244.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: 0.1302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 18/18 [00:00<00:00, 250.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: 0.1292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 18/18 [00:00<00:00, 310.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: 0.1278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 18/18 [00:00<00:00, 261.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: 0.1270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 18/18 [00:00<00:00, 239.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: 0.1266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 18/18 [00:00<00:00, 268.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: 0.1266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 18/18 [00:00<00:00, 292.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: 0.1257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 18/18 [00:00<00:00, 299.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: 0.1261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2305005606677797,\n",
              " 0.18900182429287168,\n",
              " 0.15895136280192268,\n",
              " 0.1459977970355087,\n",
              " 0.1417025596731239,\n",
              " 0.13765139629443487,\n",
              " 0.1361824195418093,\n",
              " 0.13419797437058556,\n",
              " 0.13257200312283304,\n",
              " 0.13318195111221737,\n",
              " 0.1321989618655708,\n",
              " 0.130135970397128,\n",
              " 0.13023176582323182,\n",
              " 0.12921922239992353,\n",
              " 0.1277505618830522,\n",
              " 0.12695572152733803,\n",
              " 0.12661020333568254,\n",
              " 0.1266499672912889,\n",
              " 0.12571345248983967,\n",
              " 0.12613738452394804]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from torch.nn import MSELoss\n",
        "\n",
        "# Train the model\n",
        "model = SimpleMLP(input_dim, hidden_dim, output_dim, num_hidden_layers, last_layer_activation_fn=nn.Sigmoid)\n",
        "criterion = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "trainer = SimpleMLPTrainer(model, criterion, optimizer)\n",
        "trainer.train(train_loader, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "val_loss, accuracy = trainer.evaluate(val_loader)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mpD9y0SBLtQ",
        "outputId": "b1fe0849-98ea-42c1-9998-86eec18f8319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1590, Accuracy: 0.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuFqMiX95hqj"
      },
      "source": [
        "### 3. NLLLoss (`torch.nn.NLLLoss`)\n",
        "- **Description:** Negative Log-Likelihood Loss measures the likelihood of the target class under the predicted probability distribution.\n",
        "- **Use Case:** Typically used in multi-class classification tasks, especially when combined with `log_softmax` activation.\n",
        "\n",
        "Here is the mathematical formulation of NLLLoss:\n",
        "\\begin{equation}\n",
        "\\text{NLLLoss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\log(y_{i})\n",
        "\\end{equation}\n",
        "\n",
        "I hope you note the logarithm in the formula. It's important!\n",
        "\n",
        "Why?\n",
        "\n",
        "In this part, run your training with Relu at last layer. <span style=\"color:red; font-weight: bold;\">Discuss </span> and explain the difference between the results of the two models. Find a proper solution to the problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "data = pd.read_csv(train_url)\n",
        "\n",
        "# Preprocessing (simple example)\n",
        "data = data[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']].dropna()\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Convert the data to PyTorch tensors and create a DataLoader\n",
        "X = data[['Pclass', 'Sex', 'Age', 'Fare']].values\n",
        "y = data['Survived'].values\n",
        "\n",
        "# We can scale the data\n",
        "# Without scaling we got an accuracy of around 62%\n",
        "# But after scaling, we observe around 15% increase!\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long).squeeze()\n",
        "\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Define the model, criterion, and optimizer\n",
        "input_dim = X.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = 2\n",
        "num_hidden_layers = 2\n"
      ],
      "metadata": {
        "id": "d5icSkGKU3mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEnRS5xa5hqj",
        "outputId": "08b4f128-f51d-4021-a47a-86a7bb37d76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 18/18 [00:00<00:00, 454.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: -0.3812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 18/18 [00:00<00:00, 524.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: -1.0876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 18/18 [00:00<00:00, 512.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: -2.4118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 18/18 [00:00<00:00, 475.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: -4.6004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 18/18 [00:00<00:00, 457.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: -8.1313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 18/18 [00:00<00:00, 447.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: -13.5618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 18/18 [00:00<00:00, 525.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: -21.3654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 18/18 [00:00<00:00, 434.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: -32.2438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 18/18 [00:00<00:00, 498.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: -46.7161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 18/18 [00:00<00:00, 485.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: -65.9813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 18/18 [00:00<00:00, 541.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: -91.3280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 18/18 [00:00<00:00, 499.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: -122.4030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 18/18 [00:00<00:00, 444.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: -160.5675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 18/18 [00:00<00:00, 484.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: -205.4757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 18/18 [00:00<00:00, 335.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: -261.0701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 18/18 [00:00<00:00, 358.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: -324.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 18/18 [00:00<00:00, 399.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: -397.8480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 18/18 [00:00<00:00, 352.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: -484.4124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 18/18 [00:00<00:00, 367.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: -582.2153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 18/18 [00:00<00:00, 384.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: -690.6703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.3811810256706344,\n",
              " -1.087620993455251,\n",
              " -2.411794937319226,\n",
              " -4.600350989235772,\n",
              " -8.131326384014553,\n",
              " -13.561817328135172,\n",
              " -21.36537233988444,\n",
              " -32.24382336934408,\n",
              " -46.71612601810031,\n",
              " -65.98131815592448,\n",
              " -91.32799127366808,\n",
              " -122.403015560574,\n",
              " -160.5674663119846,\n",
              " -205.47572326660156,\n",
              " -261.0700624254015,\n",
              " -324.1818440755208,\n",
              " -397.84795633951825,\n",
              " -484.4124281141493,\n",
              " -582.2153371175131,\n",
              " -690.6703389485677]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Run with relu activation function\n",
        "from torch.nn import NLLLoss\n",
        "\n",
        "# Train the model\n",
        "relu_model = SimpleMLP(input_dim, hidden_dim, output_dim, num_hidden_layers, last_layer_activation_fn=nn.ReLU)\n",
        "relu_criterion = NLLLoss()\n",
        "relu_optimizer = Adam(relu_model.parameters(), lr=0.001)\n",
        "relu_trainer = SimpleMLPTrainer(relu_model, relu_criterion, relu_optimizer)\n",
        "relu_trainer.train(train_loader, num_epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4aRT_s-5hqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7632ed43-9426-4972-9818-1a757436c327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: -754.2925, Accuracy: 0.61%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "relu_val_loss, relu_val_acc = relu_trainer.evaluate(val_loader)\n",
        "print(f\"Validation Loss: {relu_val_loss:.4f}, Accuracy: {relu_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import LogSoftmax\n",
        "\n",
        "logsoftmax_model = SimpleMLP(input_dim, hidden_dim, output_dim, num_hidden_layers, last_layer_activation_fn=LogSoftmax)\n",
        "logsoftmax_criterion = NLLLoss()\n",
        "logsoftmax_optimizer = Adam(logsoftmax_model.parameters(), lr=0.001)\n",
        "logsoftmax_trainer = SimpleMLPTrainer(logsoftmax_model, logsoftmax_criterion, logsoftmax_optimizer)\n",
        "\n",
        "\n",
        "logsoftmax_trainer.train(train_loader, num_epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meHDNgYRa-XT",
        "outputId": "9f9d8df0-f356-4f9c-a393-0f4846425923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   0%|          | 0/18 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "Epoch 1/20: 100%|██████████| 18/18 [00:00<00:00, 395.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.6360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 18/18 [00:00<00:00, 519.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: 0.5148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 18/18 [00:00<00:00, 547.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: 0.4497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 18/18 [00:00<00:00, 528.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: 0.4309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 18/18 [00:00<00:00, 522.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 0.4198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 18/18 [00:00<00:00, 497.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: 0.4147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 18/18 [00:00<00:00, 481.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: 0.4149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 18/18 [00:00<00:00, 439.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: 0.4100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 18/18 [00:00<00:00, 463.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: 0.4078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 18/18 [00:00<00:00, 546.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: 0.4057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 18/18 [00:00<00:00, 554.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: 0.4049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 18/18 [00:00<00:00, 490.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: 0.4013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 18/18 [00:00<00:00, 444.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: 0.4019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 18/18 [00:00<00:00, 441.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: 0.4002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 18/18 [00:00<00:00, 436.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: 0.3968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 18/18 [00:00<00:00, 404.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: 0.3956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 18/18 [00:00<00:00, 388.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: 0.3969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 18/18 [00:00<00:00, 502.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: 0.3957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 18/18 [00:00<00:00, 478.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: 0.3961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 18/18 [00:00<00:00, 395.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: 0.4013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6359966331058078,\n",
              " 0.5147975318961673,\n",
              " 0.4497317141956753,\n",
              " 0.4308544993400574,\n",
              " 0.4198215852181117,\n",
              " 0.41474545664257473,\n",
              " 0.41493046780427295,\n",
              " 0.410004648897383,\n",
              " 0.4078483581542969,\n",
              " 0.4056989981068505,\n",
              " 0.4048559640844663,\n",
              " 0.4012915649347835,\n",
              " 0.401866614818573,\n",
              " 0.4002491450972027,\n",
              " 0.3967904663748211,\n",
              " 0.39564831886026597,\n",
              " 0.3968635035885705,\n",
              " 0.39569133188989425,\n",
              " 0.3961053142944972,\n",
              " 0.40127600563897026]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logsoftmax_val_loss, logsoftmax_val_acc = logsoftmax_trainer.evaluate(val_loader)\n",
        "print(f\"Validation Loss: {logsoftmax_val_loss:.4f}, Accuracy: {logsoftmax_val_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enIyH7s1bIMX",
        "outputId": "0718d8e3-9e69-4e89-a967-91a179e52ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.4816, Accuracy: 0.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we used Relu as the activation function of the last layer. We had to change `output_dim` from 1 to 2. The two output nodes represent the scores (logits) for the two classes (Survived = 0 or 1) and work well with the NLLLoss function, which expects such a multi-class structure. Without this change, the model would output a single scalar value, incompatible with multi-class classification.\\\n",
        "Also, we had to change y tensor’s data type and shape: We set y to torch.long because NLLLoss requires the target tensor to be in a long integer format representing class indices (0 or 1). The .squeeze() ensures that y is 1D, with each entry corresponding to the class label. Previously, with torch.float32 and .unsqueeze(1), y was formatted as a 2D tensor suitable for regression or binary classification, but incompatible with NLLLoss, which expects 1D integer labels for multi-class classification.\\\n",
        "The model is producing low validation accuracy and an large negative loss value. These results suggest that the model setup is not quite compatible with the NLLLoss function and the ReLU activation in the output layer. The reasons are:\n",
        "- The NLLLoss expects log-probabilities as input (typically produced by a LogSoftmax layer).\n",
        "Since ReLU outputs non-negative values, it’s producing unexpected log-probabilities for NLLLoss, resulting in invalid loss values (negative numbers) and poor model performance (close to random guessing).\n",
        "- By using output_dim=2 and setting y to long integers, the model avoided previous errors by aligning with the expected input shape and data type for NLLLoss.\n",
        "However, this was only a partial solution, as the underlying issue—using ReLU on the final layer with NLLLoss—wasn’t addressed. This is why the current model does not perform well.\n"
      ],
      "metadata": {
        "id": "BvUbWAZyYx77"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy1iKwqY5hqk"
      },
      "source": [
        "**Your reason for your choice:**\\\n",
        "To make this configuration compatible with NLLLoss, we should replace the ReLU activation with a LogSoftmax layer. This will provide log-probabilities needed by NLLLoss, yielding accurate loss values and higher accuracy.\\\n",
        "To improve the model, we’ll implement the following changes:\n",
        "We replace ReLU Activation with LogSoftmax at Output:\n",
        "\n",
        "Instead of ReLU on the last layer, we’ll apply LogSoftmax to convert the outputs into log-probabilities.\n",
        "This will allow NLLLoss to calculate loss properly, which should lead to meaningful improvements in both loss and accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoNBdd-c5hqk"
      },
      "source": [
        "\n",
        "### 4. CrossEntropyLoss (`torch.nn.CrossEntropyLoss`)\n",
        "- **Description:** Combines `LogSoftmax` and `NLLLoss` in one single class. It computes the cross-entropy loss between the target and the output logits.\n",
        "- **Use Case:** Widely used for multi-class classification problems.\n",
        "\n",
        "The mathematical formulation of CrossEntropyLoss is as follows:\n",
        "\\begin{equation}\n",
        "  \\text{CrossEntropy}(y, \\hat{y}) = - \\sum_{i=1}^{C} y_i \\log\\left(\\frac{e^{\\hat{y}_i}}{\\sum_{j=1}^{C} e^{\\hat{y}_j}}\\right)\n",
        "\\end{equation}\n",
        "  where:\n",
        "  - \\( C \\) is the number of classes,\n",
        "  - \\( y_i \\) is a one-hot encoded target vector (or a scalar class label),\n",
        "  - \\( \\hat{y}_i \\) represents the logits (unnormalized model outputs) for each class.\n",
        "  \n",
        "  In practice, `torch.nn.CrossEntropyLoss` expects raw logits as input and internally applies the softmax function to convert the logits into probabilities, followed by the negative log-likelihood computation.\n",
        "\n",
        "- **Background:** Cross-entropy measures the difference between the true distribution \\( y \\) and the predicted distribution \\( \\hat{y} \\). The function minimizes the negative log-probability assigned to the correct class, effectively penalizing predictions that deviate from the true class, making it a standard choice for classification tasks in deep learning.\n",
        "\n",
        "Now, let's implement a class called `SimpleMLP_Loss` that has the following architecture:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zokhzsG15hql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e3bf05-e786-4b66-df11-0fa09ba739b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 18/18 [00:00<00:00, 546.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.6154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 18/18 [00:00<00:00, 535.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: 0.5011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 18/18 [00:00<00:00, 544.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: 0.4501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 18/18 [00:00<00:00, 496.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: 0.4318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 18/18 [00:00<00:00, 540.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 0.4230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 18/18 [00:00<00:00, 499.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: 0.4169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 18/18 [00:00<00:00, 458.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: 0.4147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 18/18 [00:00<00:00, 482.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: 0.4149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 18/18 [00:00<00:00, 501.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: 0.4053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 18/18 [00:00<00:00, 369.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: 0.4060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 18/18 [00:00<00:00, 389.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: 0.4019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 18/18 [00:00<00:00, 396.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: 0.4033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 18/18 [00:00<00:00, 318.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: 0.4017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 18/18 [00:00<00:00, 378.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: 0.3990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 18/18 [00:00<00:00, 361.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: 0.3986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 18/18 [00:00<00:00, 419.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: 0.3965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 18/18 [00:00<00:00, 403.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: 0.3956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 18/18 [00:00<00:00, 419.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: 0.3961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 18/18 [00:00<00:00, 579.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: 0.3923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 18/18 [00:00<00:00, 535.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: 0.3919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6154282457298703,\n",
              " 0.5010942932632234,\n",
              " 0.4501282705201043,\n",
              " 0.43184634877575767,\n",
              " 0.42301610277758706,\n",
              " 0.4169138918320338,\n",
              " 0.4147384473019176,\n",
              " 0.4149402495887544,\n",
              " 0.40525172154108685,\n",
              " 0.4060488889614741,\n",
              " 0.40193768342336017,\n",
              " 0.4032685160636902,\n",
              " 0.4017352991633945,\n",
              " 0.3990241040786107,\n",
              " 0.3986136598719491,\n",
              " 0.3964868701166577,\n",
              " 0.39562932319111294,\n",
              " 0.3961096637778812,\n",
              " 0.3922592310441865,\n",
              " 0.39192621989382637]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Load dataset\n",
        "train_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "data = pd.read_csv(train_url)\n",
        "\n",
        "# Preprocessing\n",
        "data = data[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']].dropna()\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Convert the data to PyTorch tensors and create a DataLoader\n",
        "X = data[['Pclass', 'Sex', 'Age', 'Fare']].values\n",
        "y = data['Survived'].values\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long).squeeze()  # Long for classification labels\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = 2\n",
        "num_hidden_layers = 2\n",
        "\n",
        "# In this part we did not apply any activation function to the last layer\n",
        "# because CrossEntropyLoss internally applies both the softmax activation and the negative log-likelihood computation\n",
        "ce_model = SimpleMLP(input_dim, hidden_dim, output_dim, num_hidden_layers, last_layer_activation_fn=None)\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "ce_optimizer = Adam(ce_model.parameters(), lr=0.001)\n",
        "ce_trainer = SimpleMLPTrainer(ce_model, ce_criterion, ce_optimizer)\n",
        "ce_trainer.train(train_loader, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ce_loss, ce_acc = ce_trainer.evaluate(val_loader)\n",
        "print(f\"Validation Loss: {ce_loss:.4f}, Accuracy: {ce_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fITVYOlpiVwk",
        "outputId": "26752f8b-97a1-4282-c420-f522e706d17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.4740, Accuracy: 0.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Ubiohl5hql"
      },
      "source": [
        "\n",
        "### 5. KLDivLoss (`torch.nn.KLDivLoss`)\n",
        "- **Description:** Kullback-Leibler Divergence Loss measures how one probability distribution diverges from a second, reference distribution. Unlike other loss functions that focus on classification, KL divergence specifically compares the relative entropy between two distributions. It quantifies the information loss when using the predicted distribution to approximate the true distribution.\n",
        "\n",
        "- **Mathematical Function:**\n",
        "\\begin{equation}\n",
        "  \\text{KL}(P \\parallel Q) = \\sum_{i=1}^{C} P(i) \\left( \\log P(i) - \\log Q(i) \\right)\n",
        "\\end{equation}\n",
        "  where:\n",
        "  - \\( P \\) is the target (true) probability distribution,\n",
        "  - \\( Q \\) is the predicted distribution (often the output of `log_softmax`),\n",
        "  - \\( C \\) is the number of classes.\n",
        "\n",
        "  KL divergence is always non-negative, and it equals zero if the two distributions are identical. The loss function expects the model's output to be in the form of log-probabilities (using `log_softmax`) and compares this against a target probability distribution, which is typically a normalized distribution (using softmax).\n",
        "\n",
        "- **Use Case:** KLDivLoss is frequently used in:\n",
        "  - **Variational Autoencoders (VAEs):** In VAEs, KL divergence is used to measure how much the learned latent space distribution deviates from a prior distribution (often Gaussian).\n",
        "  - **Knowledge Distillation:** In teacher-student models, KL divergence is used to transfer the \"soft\" knowledge from a teacher model to a student model by comparing their output probability distributions.\n",
        "  - **Reinforcement Learning:** It can be used to update policies while minimizing the divergence from a previous policy.\n",
        "\n",
        "- **Background:** Kullback-Leibler divergence, a core concept in information theory, measures the inefficiency of assuming the predicted distribution \\( Q \\) when the true distribution is \\( P \\). It is asymmetric, meaning that \\( KL(P \\parallel Q) \\neq KL(Q \\parallel P) \\), so the direction of the comparison matters.\n",
        "\n",
        "Again, in this part, run your training with Relu at last layer. <span style=\"color:red; font-weight: bold;\">Discuss </span> and explain the difference between the results of the two models. Find a proper solution to the problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "# Load dataset\n",
        "train_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "data = pd.read_csv(train_url)\n",
        "\n",
        "# Preprocessing (simple example)\n",
        "data = data[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']].dropna()\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Convert the data to PyTorch tensors and create a DataLoader\n",
        "X = data[['Pclass', 'Sex', 'Age', 'Fare']].values\n",
        "y = data['Survived'].values\n",
        "\n",
        "# We can scale the data\n",
        "# Without scaling we got an accuracy of around 62%\n",
        "# But after scaling, we observe around 15% increase!\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "y = F.one_hot(y, num_classes=2).float()\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Define the model, criterion, and optimizer\n",
        "input_dim = X.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = 2\n",
        "num_hidden_layers = 2"
      ],
      "metadata": {
        "id": "l-ppLTjvwMyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ykXXxpa5hql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429df109-77e3-4ec1-cc06-ebc214188be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 18/18 [00:00<00:00, 527.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: -0.3056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 18/18 [00:00<00:00, 499.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: -1.0055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 18/18 [00:00<00:00, 520.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: -2.4310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 18/18 [00:00<00:00, 498.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: -5.1360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 18/18 [00:00<00:00, 440.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: -9.8650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 18/18 [00:00<00:00, 508.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: -17.5313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 18/18 [00:00<00:00, 534.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: -29.3701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 18/18 [00:00<00:00, 435.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: -46.2668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 18/18 [00:00<00:00, 368.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: -69.5050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 18/18 [00:00<00:00, 271.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: -100.2279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 18/18 [00:00<00:00, 273.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: -139.4075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 18/18 [00:00<00:00, 238.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: -187.9260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 18/18 [00:00<00:00, 213.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: -248.6599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 18/18 [00:00<00:00, 300.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: -320.7862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 18/18 [00:00<00:00, 274.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: -406.8640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 18/18 [00:00<00:00, 391.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: -507.8099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 18/18 [00:00<00:00, 309.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: -623.8188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 18/18 [00:00<00:00, 344.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: -756.4103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 18/18 [00:00<00:00, 350.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: -906.5984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 18/18 [00:00<00:00, 302.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: -1077.3354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.3056059243778388,\n",
              " -1.0054754780398474,\n",
              " -2.4310453004307218,\n",
              " -5.13602344195048,\n",
              " -9.865025361378988,\n",
              " -17.531318134731716,\n",
              " -29.370090378655327,\n",
              " -46.26676517062717,\n",
              " -69.50498729281955,\n",
              " -100.22786670260959,\n",
              " -139.40747451782227,\n",
              " -187.92595418294272,\n",
              " -248.65987565782336,\n",
              " -320.7862074110243,\n",
              " -406.8639899359809,\n",
              " -507.80985683865015,\n",
              " -623.818834092882,\n",
              " -756.4102952745226,\n",
              " -906.598375108507,\n",
              " -1077.3354187011719]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Run with relu activation function\n",
        "from torch.nn import KLDivLoss\n",
        "\n",
        "relu_model = SimpleMLP(input_dim, hidden_dim, output_dim, num_hidden_layers, last_layer_activation_fn=nn.ReLU)\n",
        "relu_criterion = KLDivLoss(reduction='batchmean')\n",
        "relu_optimizer = Adam(relu_model.parameters(), lr=0.001)\n",
        "\n",
        "relu_trainer = SimpleMLPTrainer(relu_model, relu_criterion, relu_optimizer)\n",
        "relu_trainer.train(train_loader, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relu_val_loss, relu_val_acc = relu_trainer.evaluate(val_loader)\n",
        "print(f\"ReLU Model - Validation Loss: {relu_val_loss:.4f}, Accuracy: {relu_val_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkKoawsovsdY",
        "outputId": "bcfa3984-3b48-4813-9b0c-aa5f133265f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU Model - Validation Loss: -1190.2690, Accuracy: 0.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxXFOH815hql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cc3805-5cae-424d-c251-0e83ad771e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   0%|          | 0/18 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "Epoch 1/20: 100%|██████████| 18/18 [00:00<00:00, 458.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.6475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 18/18 [00:00<00:00, 503.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: 0.5340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 18/18 [00:00<00:00, 542.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: 0.4636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 18/18 [00:00<00:00, 445.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: 0.4354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 18/18 [00:00<00:00, 377.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 0.4249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 18/18 [00:00<00:00, 298.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: 0.4197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 18/18 [00:00<00:00, 428.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: 0.4156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 18/18 [00:00<00:00, 486.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: 0.4112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 18/18 [00:00<00:00, 496.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: 0.4088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 18/18 [00:00<00:00, 482.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: 0.4113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 18/18 [00:00<00:00, 445.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: 0.4045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 18/18 [00:00<00:00, 459.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: 0.4053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 18/18 [00:00<00:00, 399.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: 0.4002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 18/18 [00:00<00:00, 315.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: 0.3979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 18/18 [00:00<00:00, 468.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: 0.3957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 18/18 [00:00<00:00, 501.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: 0.3953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 18/18 [00:00<00:00, 476.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: 0.3966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 18/18 [00:00<00:00, 459.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: 0.3911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 18/18 [00:00<00:00, 453.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: 0.3913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 18/18 [00:00<00:00, 474.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: 0.3901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6474657754103342,\n",
              " 0.5339570790529251,\n",
              " 0.4635663777589798,\n",
              " 0.4353876925177044,\n",
              " 0.42491303715440965,\n",
              " 0.41970115154981613,\n",
              " 0.41560972068044877,\n",
              " 0.4111771550443437,\n",
              " 0.40882642898294663,\n",
              " 0.41129814916186863,\n",
              " 0.4045010606447856,\n",
              " 0.405258693628841,\n",
              " 0.40015405664841336,\n",
              " 0.3978937218586604,\n",
              " 0.39567310197485817,\n",
              " 0.39528197960721123,\n",
              " 0.39655165870984393,\n",
              " 0.3910642655359374,\n",
              " 0.3912660694784588,\n",
              " 0.3901420657833417]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Run with softmax activation function\n",
        "from torch.nn import LogSoftmax, KLDivLoss\n",
        "\n",
        "logsoftmax_model = SimpleMLP(input_dim, hidden_dim, output_dim, num_hidden_layers, last_layer_activation_fn=LogSoftmax)\n",
        "logsoftmax_criterion = KLDivLoss(reduction='batchmean')\n",
        "logsoftmax_optimizer = Adam(logsoftmax_model.parameters(), lr=0.001)\n",
        "\n",
        "logsoftmax_trainer = SimpleMLPTrainer(logsoftmax_model, logsoftmax_criterion, logsoftmax_optimizer)\n",
        "logsoftmax_trainer.train(train_loader, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logsoftmax_val_loss, logsoftmax_val_acc = logsoftmax_trainer.evaluate(val_loader)\n",
        "print(f\"LogSoftmax Model - Validation Loss: {logsoftmax_val_loss:.4f}, Accuracy: {logsoftmax_val_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDhe_cI9w2Ex",
        "outputId": "bd7aacd2-b56d-4bba-be6d-b2aec657b77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogSoftmax Model - Validation Loss: 0.4742, Accuracy: 0.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhhWjO795hql"
      },
      "source": [
        "**Your reason for your choice:**\n",
        "\n",
        "KLDivLoss requires the output from the model to be in the form of log-probabilities (achieved using LogSoftmax), and the target labels to be probability distributions rather than integer class labels. Initially, we faced two main issues:\n",
        "\n",
        "- ReLU activation on the final layer did not produce the log-probabilities required by KLDivLoss, causing the loss function to interpret outputs incorrectly.\n",
        "- Integer class labels were incompatible with KLDivLoss, which expects probability distributions.\n",
        "\n",
        "To resolve these issues, we implemented the following steps:\n",
        "\n",
        "- Replaced ReLU with LogSoftmax on the last layer, which transformed the model's outputs into log-probabilities. This made the output suitable for KLDivLoss.\n",
        "- One-hot encoded the target labels into probability distributions (e.g., [1, 0] or [0, 1] for binary classes), aligning them with KLDivLoss’s expectation for target distribution inputs.\n",
        "\n",
        "The results of training with ReLU vs. LogSoftmax activation in the final layer were starkly different:\n",
        "\n",
        "ReLU Model:\n",
        "\n",
        "The ReLU model produced highly negative validation loss values. This occurred because ReLU does not produce log-probabilities, so KLDivLoss received improper inputs, causing it to calculate divergence inaccurately. The model’s accuracy also stagnated at around 61%, indicating poor alignment between the predictions and target labels.\n",
        "\n",
        "LogSoftmax Model:\n",
        "\n",
        "\n",
        "With LogSoftmax as the final layer activation, the validation loss stabilized at a lower value, which is a reasonable value for KLDivLoss, and the model achieved a higher accuracy. LogSoftmax ensured the model's outputs were log-probabilities, enabling KLDivLoss to calculate the divergence correctly, significantly improving model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYx81yUC5hql"
      },
      "source": [
        "### 6. CosineEmbeddingLoss (`torch.nn.CosineEmbeddingLoss`)\n",
        "- **Description:** Measures the cosine similarity between two input tensors, `x1` and `x2`, and computes the loss based on a label `y` that indicates whether the tensors should be similar (`y = 1`) or dissimilar (`y = -1`). Cosine similarity focuses on the angle between vectors, disregarding their magnitude.\n",
        "\n",
        "- **Mathematical Function:**\n",
        "\\begin{equation}\n",
        "  \\text{CosineEmbeddingLoss}(x1, x2, y) =\n",
        "  \\begin{cases}\n",
        "  1 - \\cos(x_1, x_2), & \\text{if } y = 1 \\\\\n",
        "  \\max(0, \\cos(x_1, x_2) - \\text{margin}), & \\text{if } y = -1\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "  where $ \\cos(x_1, x_2) $ is the cosine similarity between the two vectors, and `margin` is a threshold that determines how dissimilar the vectors should be.\n",
        "\n",
        "- **Use Case:** Commonly used in tasks like face verification, image similarity, and other scenarios where the relative orientation of vectors (angle) is more important than their length, such as in embeddings and metric learning.\n",
        "\n",
        "- **Background:** Cosine similarity compares the directional alignment of vectors, making it ideal for high-dimensional data where the magnitude may not be as informative. This loss is particularly useful when training models to learn meaningful embeddings that capture semantic similarity.\n",
        "\n",
        "You'll become more fimiliar with this loss function in future.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BdUfw6N5hql"
      },
      "source": [
        "# Regularization in Machine Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Regularization is a fundamental technique in machine learning that helps prevent overfitting by adding a penalty to the loss function. This penalty discourages the model from becoming too complex, ensuring better generalization to unseen data. In this notebook, you will explore the concepts of regularization, understand different types of regularization techniques, and apply them using Python's popular libraries.\n",
        "\n",
        "## What is Regularization?\n",
        "\n",
        "Regularization involves adding a regularization term to the loss function used to train machine learning models. This term imposes a constraint on the model's coefficients, effectively reducing their magnitude. By doing so, regularization helps in:\n",
        "\n",
        "- **Preventing Overfitting:** Ensures the model does not become too tailored to the training data.\n",
        "- **Improving Generalization:** Enhances the model's performance on new, unseen data.\n",
        "- **Feature Selection:** Especially in L1 regularization, it can drive some coefficients to zero, effectively selecting important features.\n",
        "\n",
        "## Types of Regularization\n",
        "\n",
        "There are several types of regularization techniques, each imposing different constraints on the model's parameters:\n",
        "\n",
        "### 1. L1 Regularization (Lasso)\n",
        "\n",
        "L1 regularization adds the absolute value of the magnitude of coefficients as a penalty term to the loss function. It can lead to sparse models where some feature coefficients are exactly zero.\n",
        "\n",
        "### 2. L2 Regularization (Ridge)\n",
        "\n",
        "L2 regularization adds the squared magnitude of coefficients as a penalty term to the loss function. It tends to shrink the coefficients evenly but does not set them to zero.\n",
        "\n",
        "### 3. Elastic Net\n",
        "\n",
        "Elastic Net combines both L1 and L2 regularization penalties. It balances the benefits of both Lasso and Ridge methods, allowing for feature selection and coefficient shrinkage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfQ5bOOA5hql"
      },
      "source": [
        "## Homework Time!\n",
        "Import Iris dataset from sklearn.datasets and apply ridge regression with different alpha values. Then, create a gif that shows the changes of the classification boundary with respect to alpha values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpG1RwHJ5hqm"
      },
      "source": [
        "Import the libs that you need and start coding!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1VVhedY5hqm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import imageio\n",
        "import warnings\n",
        "\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV0WGfak5hqm"
      },
      "source": [
        "Load the Iris dataset and select Setosa and Versicolor classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB9RFDg-5hqm"
      },
      "outputs": [],
      "source": [
        "# 1. Load and Prepare the Iris Dataset\n",
        "\n",
        "# Select only two classes for binary classification (Setosa and Versicolor)\n",
        "iris = load_iris()\n",
        "\n",
        "# Select two features for 2D visualization (Sepal Length and Petal Length)\n",
        "X = iris.data[iris.target != 2][:, [0, 2]]\n",
        "y = iris.target[iris.target != 2]\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-V7-7he5hqm"
      },
      "source": [
        "Define Function to Plot Decision Boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_63svNQ5hqm"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(model, X, y, alpha):\n",
        "    # Define the grid (use meshgrid)\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "\n",
        "\n",
        "    # Predict over the grid\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Create a figure\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, levels=[-0.1, 0.1, 1.1], colors=['blue', 'red'])\n",
        "\n",
        "    # Scatter plot of the training data\n",
        "    scatter = ax.scatter(\n",
        "        X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', s=50\n",
        "    )\n",
        "\n",
        "    # Title and labels\n",
        "    ax.set_title(f'MLP Decision Boundary (alpha={alpha})')\n",
        "    ax.set_xlabel('Sepal Length (standardized)')\n",
        "    ax.set_ylabel('Petal Length (standardized)')\n",
        "\n",
        "    # Remove axes for clarity\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    # Tight layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot to a BytesIO object\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    return Image.open(buf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qd-PwLU5hqm"
      },
      "source": [
        "Train MLP with Varying Alpha Values and Collect Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3oqM0Js5hqm"
      },
      "outputs": [],
      "source": [
        "def create_decision_boundary_gif(alpha_values, X_train, y_train, n_neurons):\n",
        "\n",
        "    # List to store images\n",
        "    images = []\n",
        "\n",
        "    for idx, alpha in enumerate(alpha_values):\n",
        "        print(f\"Processing alpha={alpha:.4f} ({idx + 1}/{len(alpha_values)})\")\n",
        "\n",
        "        # Create and train the MLP\n",
        "        mlp = MLPClassifier(\n",
        "            hidden_layer_sizes=(n_neurons,),\n",
        "            alpha=alpha,\n",
        "            max_iter=1000,\n",
        "            random_state=42\n",
        "        )\n",
        "        mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "        # Plot decision boundary and get the image\n",
        "        img = plot_decision_boundary(mlp, X_train, y_train, alpha)\n",
        "        images.append(img)\n",
        "\n",
        "    # Save the images as a GIF\n",
        "    gif_filename = 'mlp_classification_boundaries.gif'\n",
        "    images[0].save(\n",
        "        gif_filename,\n",
        "        save_all=True,\n",
        "        append_images=images[1:],\n",
        "        duration=500,\n",
        "        loop=0\n",
        "    )\n",
        "\n",
        "    print(f\"GIF saved as '{gif_filename}'\")\n",
        "\n",
        "    # return the gif\n",
        "    return gif_filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "478iqnsl5hqm"
      },
      "source": [
        "## RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoFzYKQT5hqm",
        "outputId": "52532179-fd52-4399-c795-ab6d762eff85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing alpha=0.0010 (1/20)\n",
            "Processing alpha=0.0021 (2/20)\n",
            "Processing alpha=0.0043 (3/20)\n",
            "Processing alpha=0.0089 (4/20)\n",
            "Processing alpha=0.0183 (5/20)\n",
            "Processing alpha=0.0379 (6/20)\n",
            "Processing alpha=0.0785 (7/20)\n",
            "Processing alpha=0.1624 (8/20)\n",
            "Processing alpha=0.3360 (9/20)\n",
            "Processing alpha=0.6952 (10/20)\n",
            "Processing alpha=1.4384 (11/20)\n",
            "Processing alpha=2.9764 (12/20)\n",
            "Processing alpha=6.1585 (13/20)\n",
            "Processing alpha=12.7427 (14/20)\n",
            "Processing alpha=26.3665 (15/20)\n",
            "Processing alpha=54.5559 (16/20)\n",
            "Processing alpha=112.8838 (17/20)\n",
            "Processing alpha=233.5721 (18/20)\n",
            "Processing alpha=483.2930 (19/20)\n",
            "Processing alpha=1000.0000 (20/20)\n",
            "GIF saved as 'mlp_classification_boundaries.gif'\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "# Use np.logspace to generate alpha values, with at least 20 values\n",
        "alpha_values = np.logspace(-3, 3, 20)  # alpha from 0.001 to 1000\n",
        "\n",
        "# Define the number of neurons in the hidden layer\n",
        "n_neurons =  10\n",
        "\n",
        "# Create the decision boundary GIF\n",
        "gif_dir = create_decision_boundary_gif(alpha_values, X_train, y_train, n_neurons)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z15ijDO65hqn"
      },
      "source": [
        "Your gif should look like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggUoqD6o5hqn"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "### **Multilayer Perceptron Classification Boundaries**\n",
        "\n",
        "![Classification Boundaries](mlp_classification_boundaries_example.gif)\n",
        "\n",
        "*Figure 1: Demonstration of classification boundaries created by a Multilayer Perceptron (MLP) model.*\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}